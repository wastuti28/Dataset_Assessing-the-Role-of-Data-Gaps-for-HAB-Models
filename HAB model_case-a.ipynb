{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a81144a-f3e4-473d-aa4d-650ff6a14c42",
   "metadata": {},
   "source": [
    "# Random Forest Regression for Harmful Algal Bloom (HAB) Dataset\n",
    "\n",
    "This script implements and evaluates a time series Random Forest regression workflow to predict HAB using environmental predictors. The analysis includes time-series data splitting, hyperparameter tuning, cross-validation, and feature importance analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451f234c-74ec-4ac8-8016-426c2f921733",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "Import all necessary libraries for machine learning, data manipulation, and model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abe2395d-ffe4-4bff-8697-470377a6ec69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f700b471-75f7-42e0-8b0e-3d072ccc4dcd",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Preparation\n",
    "Load the dataset and examine its structure. The dataset contains environmental variables that may influence HAB abundance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3fa1b7d-fe68-404c-aacb-924dbdc78d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 85 samples\n",
      "Dataset shape: (85, 13)\n",
      "\n",
      "First few rows:\n",
      "        Time  Solar Rad   Air Temp   Precpt  Wind Speed  Water Temp  \\\n",
      "0 2016-04-01      609.0  23.398667   16.774    5.111161      24.366   \n",
      "1 2016-05-01      627.0  25.310000   95.484    4.405007      26.042   \n",
      "2 2016-06-01      552.0  27.379000  139.785    4.269408      29.262   \n",
      "3 2016-07-01      602.0  28.761000   63.011    3.643123      31.428   \n",
      "4 2016-08-01      527.0  28.104000  172.473    4.420653      30.567   \n",
      "\n",
      "   Turbidity      DO       TN      TP   Chl-a        CyanHAB        Lake  \n",
      "0     49.475  8.4325  1.63500  0.0600  10.290   19910.620328  Okeechobee  \n",
      "1     39.100  7.8525  1.36250  0.0270  14.265   21639.256122  Okeechobee  \n",
      "2     23.025  8.3275  1.52250  0.0675  38.300  313020.993949  Okeechobee  \n",
      "3     18.875  6.9725  1.28250  0.0685  34.050  196983.234522  Okeechobee  \n",
      "4     15.150  7.3975  1.14575  0.0450  36.930   74421.112524  Okeechobee  \n",
      "Dataset size: 85 samples\n",
      "Number of features: 10\n",
      "\n",
      "Feature columns: ['Solar Rad', 'Air Temp', 'Precpt', 'Wind Speed', 'Water Temp', 'Turbidity', 'DO', 'TN', 'TP', 'Chl-a']\n",
      "Target variable: CyanHAB\n"
     ]
    }
   ],
   "source": [
    "# 2.1 Data Loading\n",
    "# Load the dataset for lake Okeecobee as an example\n",
    "data = pd.read_excel('Okeechobee.xlsx', header=0)\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(f\"Dataset size: {len(data)} samples\")\n",
    "print(f\"Dataset shape: {data.shape}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(data.head())\n",
    "\n",
    "\n",
    "# 2.1 Separate the features (environmental variables) from the target variable (HAB abundance).\n",
    "# Exclude non-predictive columns like Time and Lake identifiers.\n",
    "X = data.drop(columns=['Time','CyanHAB','Lake'], axis=1)\n",
    "y = data['CyanHAB']\n",
    "\n",
    "print(f\"Dataset size: {len(data)} samples\")\n",
    "print(f\"Number of features: {X.shape[1]}\")\n",
    "print(f\"\\nFeature columns: {list(X.columns)}\")\n",
    "print(f\"Target variable: CyanHAB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6f13fa-bf30-4342-9f25-93e0204ca519",
   "metadata": {},
   "source": [
    "## 3. Time-Based Train-Test Split\n",
    "For time series data, it's crucial to preserve temporal order. We use the most recent 30% of data for testing to evaluate how well the model predicts future HAB abundance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1770d3e-fd00-4054-87d1-8086af695e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 59 samples (69.4%)\n",
      "Test set size: 26 samples (30.6%)\n",
      "Training period: Index 0 to 58\n",
      "Test period: Index 59 to 84\n"
     ]
    }
   ],
   "source": [
    "# Splits the data chronologically: first 70% for training, last 30% for testingâ€”preserving time order.\n",
    "split_idx = int(len(data) * 0.7)\n",
    "X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n",
    "\n",
    "print(f\"Training set size: {len(X_train)} samples ({len(X_train)/len(data)*100:.1f}%)\")\n",
    "print(f\"Test set size: {len(X_test)} samples ({len(X_test)/len(data)*100:.1f}%)\")\n",
    "print(f\"Training period: Index {0} to {split_idx-1}\")\n",
    "print(f\"Test period: Index {split_idx} to {len(data)-1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9e0db6-920d-437a-8acc-ee7b16569b3f",
   "metadata": {},
   "source": [
    "## 4. Cross-Validation Setup\n",
    "Set up TimeSeriesSplit for cross-validation, which respects the temporal nature of the data by only using past data to predict future values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "888d4952-2c24-42e6-aa57-3424cf06a0bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Series Cross-Validation Setup:\n",
      "Number of CV folds: 3\n",
      "Each fold uses progressively more training data to predict the next time period\n"
     ]
    }
   ],
   "source": [
    "# User can chnge the number of split\n",
    "tscv = TimeSeriesSplit(n_splits=3)\n",
    "\n",
    "print(\"Time Series Cross-Validation Setup:\")\n",
    "print(f\"Number of CV folds: {tscv.n_splits}\")\n",
    "print(\"Each fold uses progressively more training data to predict the next time period\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40c1741-623c-48f7-86ef-fdfffd7a96a2",
   "metadata": {},
   "source": [
    "## 5. Hyperparameter Grid Definition\n",
    "Define a simplified parameter grid to reduce overfitting risk while still exploring important hyperparameters for Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8e46ed6-ce2d-472b-b0fc-cc2ee416ff98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameter Grid:\n",
      "  n_estimators: [50, 100]\n",
      "  max_depth: [10, 20, None]\n",
      "  min_samples_split: [2, 5]\n",
      "  min_samples_leaf: [5, 10]\n",
      "  max_features: ['sqrt', 0.5]\n",
      "  bootstrap: [True]\n",
      "\n",
      "Total parameter combinations: 48\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [50, 100],\n",
    "    'max_depth': [10, 20, None],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [5, 10],\n",
    "    'max_features': ['sqrt', 0.5],\n",
    "    'bootstrap': [True]  # Keep bootstrap for variance reduction\n",
    "}\n",
    "\n",
    "print(\"Hyperparameter Grid:\")\n",
    "for param, values in param_grid.items():\n",
    "    print(f\"  {param}: {values}\")\n",
    "    \n",
    "total_combinations = np.prod([len(v) for v in param_grid.values()])\n",
    "print(f\"\\nTotal parameter combinations: {total_combinations}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e45706c-0f58-4eea-8992-268ad2de2814",
   "metadata": {},
   "source": [
    "## 6. Model Training and Hyperparameter Tuning\n",
    "Perform grid search with time series cross-validation to find the best hyperparameters for predicting HAB abundance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f929c3ec-4970-4ca0-b6b6-f397e4861bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting hyperparameter tuning...\n",
      "Fitting 3 folds for each of 48 candidates, totalling 144 fits\n",
      "\n",
      "==================================================\n",
      "HYPERPARAMETER TUNING RESULTS\n",
      "==================================================\n",
      "Best Parameters: {'bootstrap': True, 'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 5, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "Best CV Score (neg_MSE): -1165521863.2197201\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter Tuning with GridSearchCV\n",
    "model = RandomForestRegressor(random_state=42)\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid,\n",
    "    scoring='neg_mean_squared_error',  # MSE often better for regression\n",
    "    cv=tscv,  # Use TimeSeriesSplit instead of KFold\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"Starting hyperparameter tuning...\")\n",
    "\n",
    "# Perform the grid search on the training set\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best hyperparameters and model\n",
    "best_model = grid_search.best_estimator_\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"HYPERPARAMETER TUNING RESULTS\")\n",
    "print(\"=\"*50)\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best CV Score (neg_MSE):\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef84449-7b49-4e02-bb93-12149b3d1bb0",
   "metadata": {},
   "source": [
    "## 7. Model Evaluation on Test Set\n",
    "Evaluate the optimized model on both training and test sets to assess performance and check for overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8129619e-a77c-48d7-9988-8256704c05dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = best_model.predict(X_test)\n",
    "y_train_pred = best_model.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782deaff-6da2-4b39-96e9-72c998c747a4",
   "metadata": {},
   "source": [
    "## 8. Feature Importance Analysis\n",
    "Analyze which environmental variables are most important for predicting HAB abundance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a8bfe0d3-43e7-4411-9f1e-3a5b75c45d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "FEATURE IMPORTANCE ANALYSIS\n",
      "==================================================\n",
      "Top 10 Most Important Features:\n",
      "      feature  importance\n",
      "4  Water Temp    0.239767\n",
      "1    Air Temp    0.164498\n",
      "9       Chl-a    0.126645\n",
      "5   Turbidity    0.122908\n",
      "2      Precpt    0.113832\n",
      "6          DO    0.072925\n",
      "8          TP    0.054559\n",
      "0   Solar Rad    0.052361\n",
      "7          TN    0.038220\n",
      "3  Wind Speed    0.014285\n"
     ]
    }
   ],
   "source": [
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': best_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FEATURE IMPORTANCE ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "print(\"Top 10 Most Important Features:\")\n",
    "print(feature_importance.head(10))\n",
    "\n",
    "# Save feature importance to Excel file\n",
    "feature_importance.to_excel('Okeechobee_feature_importance.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f698eb51-0403-4c6f-8a3d-bee99f6cfb67",
   "metadata": {},
   "source": [
    "## 9. Save The Result\n",
    "Store the prediction results for further analysis (multiple matrix: R2, RMSE, NSE etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d180662-4b9c-4e69-8d89-0544ca78f903",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.DataFrame({'Actual': y_test, 'Predicted': y_test_pred})\n",
    "df_train = pd.DataFrame({'Actual': y_train, 'Predicted': y_train_pred})\n",
    "\n",
    "# Save the result into Excel file\n",
    "df_test.to_excel('Okeechobee_test.xlsx', index=False)\n",
    "df_train.to_excel('Okeechobee_train.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
